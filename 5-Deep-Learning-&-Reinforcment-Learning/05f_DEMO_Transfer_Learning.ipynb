{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 5, Part g: Transfer Learning DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n","\n","Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import datetime\n","#import keras\n","#from keras.datasets import mnist\n","#from keras.models import Sequential\n","#from keras.layers import Dense, Dropout, Activation, Flatten\n","#from keras.layers import Conv2D, MaxPooling2D\n","#from keras import backend as K\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras import backend as K"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#used to help some of the timing functions\n","now = datetime.datetime.now"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# set some parameters\n","batch_size = 128\n","num_classes = 5\n","epochs = 5"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# set some more parameters\n","img_rows, img_cols = 28, 28\n","filters = 32\n","pool_size = 2\n","kernel_size = 3"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["## This just handles some variability in how the input data is loaded\n","\n","if K.image_data_format() == 'channels_first':\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    input_shape = (img_rows, img_cols, 1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["## To simplify things, write a function to include all the training steps\n","## As input, function takes a model, training set, test set, and the number of classes\n","## Inside the model object will be the state about which layers we are freezing and which we are training\n","\n","def train_model(model, train, test, num_classes):\n","    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n","    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n","    x_train = x_train.astype('float32')\n","    x_test = x_test.astype('float32')\n","    x_train /= 255\n","    x_test /= 255\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","\n","    # convert class vectors to binary class matrices\n","    y_train = keras.utils.to_categorical(train[1], num_classes)\n","    y_test = keras.utils.to_categorical(test[1], num_classes)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adadelta',\n","                  metrics=['accuracy'])\n","\n","    t = now()\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              verbose=1,\n","              validation_data=(x_test, y_test))\n","    print('Training time: %s' % (now() - t))\n","\n","    score = model.evaluate(x_test, y_test, verbose=0)\n","    print('Test score:', score[0])\n","    print('Test accuracy:', score[1])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# the data, shuffled and split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# create two datasets: one with digits below 5 and one with 5 and above\n","x_train_lt5 = x_train[y_train < 5]\n","y_train_lt5 = y_train[y_train < 5]\n","x_test_lt5 = x_test[y_test < 5]\n","y_test_lt5 = y_test[y_test < 5]\n","\n","x_train_gte5 = x_train[y_train >= 5]\n","y_train_gte5 = y_train[y_train >= 5] - 5\n","x_test_gte5 = x_test[y_test >= 5]\n","y_test_gte5 = y_test[y_test >= 5] - 5"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"]}],"source":["# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n","# to a new problem.  We will freeze these layers during the fine-tuning process\n","\n","feature_layers = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n","# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n","\n","classification_layers = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# We create our model by combining the two sets of layers as follows\n","model = Sequential(feature_layers + classification_layers)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," activation (Activation)     (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_1 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               589952    \n","                                                                 \n"," activation_2 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_3 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 600165 (2.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Let's take a look\n","model.summary()"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","Epoch 1/5\n","WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n","\n","WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n","\n","230/230 [==============================] - 12s 46ms/step - loss: 1.5985 - accuracy: 0.2474 - val_loss: 1.5784 - val_accuracy: 0.3662\n","Epoch 2/5\n","230/230 [==============================] - 10s 44ms/step - loss: 1.5758 - accuracy: 0.3043 - val_loss: 1.5507 - val_accuracy: 0.5289\n","Epoch 3/5\n","230/230 [==============================] - 10s 43ms/step - loss: 1.5485 - accuracy: 0.3793 - val_loss: 1.5206 - val_accuracy: 0.6095\n","Epoch 4/5\n","230/230 [==============================] - 10s 44ms/step - loss: 1.5213 - accuracy: 0.4432 - val_loss: 1.4876 - val_accuracy: 0.6651\n","Epoch 5/5\n","230/230 [==============================] - 12s 53ms/step - loss: 1.4897 - accuracy: 0.5027 - val_loss: 1.4508 - val_accuracy: 0.7021\n","Training time: 0:00:53.994513\n","Test score: 1.4507558345794678\n","Test accuracy: 0.7021189332008362\n"]}],"source":["# Now, let's train our model on the digits 5,6,7,8,9\n","\n","train_model(model,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Freezing Layers\n","Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n","\n","Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set.\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Freeze only the feature layers\n","for l in feature_layers:\n","    l.trainable = False"]},{"cell_type":"markdown","metadata":{},"source":["Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*.\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," activation (Activation)     (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_1 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               589952    \n","                                                                 \n"," activation_2 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_3 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 590597 (2.25 MB)\n","Non-trainable params: 9568 (37.38 KB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 6s 22ms/step - loss: 1.5684 - accuracy: 0.3180 - val_loss: 1.5347 - val_accuracy: 0.4622\n","Epoch 2/5\n","240/240 [==============================] - 6s 25ms/step - loss: 1.5210 - accuracy: 0.4120 - val_loss: 1.4843 - val_accuracy: 0.5657\n","Epoch 3/5\n","240/240 [==============================] - 5s 22ms/step - loss: 1.4760 - accuracy: 0.4987 - val_loss: 1.4335 - val_accuracy: 0.6811\n","Epoch 4/5\n","240/240 [==============================] - 6s 23ms/step - loss: 1.4285 - accuracy: 0.5728 - val_loss: 1.3820 - val_accuracy: 0.7690\n","Epoch 5/5\n","240/240 [==============================] - 5s 20ms/step - loss: 1.3829 - accuracy: 0.6341 - val_loss: 1.3305 - val_accuracy: 0.8332\n","Training time: 0:00:27.444152\n","Test score: 1.3304808139801025\n","Test accuracy: 0.833236038684845\n"]}],"source":["train_model(model,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n","\n","Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise\n","- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9.\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_4 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_5 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_6 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_7 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 600165 (2.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Create layers and define the model as above\n","feature_layers2 = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]\n","\n","classification_layers2 = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]\n","model2 = Sequential(feature_layers2 + classification_layers2)\n","model2.summary()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 12s 46ms/step - loss: 1.6143 - accuracy: 0.2215 - val_loss: 1.5892 - val_accuracy: 0.3545\n","Epoch 2/5\n","240/240 [==============================] - 11s 48ms/step - loss: 1.5761 - accuracy: 0.3138 - val_loss: 1.5499 - val_accuracy: 0.4406\n","Epoch 3/5\n","240/240 [==============================] - 11s 44ms/step - loss: 1.5391 - accuracy: 0.3956 - val_loss: 1.5066 - val_accuracy: 0.5355\n","Epoch 4/5\n","240/240 [==============================] - 11s 44ms/step - loss: 1.4959 - accuracy: 0.4714 - val_loss: 1.4558 - val_accuracy: 0.6406\n","Epoch 5/5\n","240/240 [==============================] - 11s 45ms/step - loss: 1.4479 - accuracy: 0.5408 - val_loss: 1.3975 - val_accuracy: 0.7320\n","Training time: 0:00:55.309406\n","Test score: 1.3974976539611816\n","Test accuracy: 0.7320490479469299\n"]}],"source":["# Now, let's train our model on the digits 0,1,2,3,4\n","train_model(model2,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#Freeze layers\n","for l in feature_layers2:\n","    l.trainable = False"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_4 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_5 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_6 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_7 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 590597 (2.25 MB)\n","Non-trainable params: 9568 (37.38 KB)\n","_________________________________________________________________\n"]}],"source":["model2.summary()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","Epoch 1/5\n","230/230 [==============================] - 5s 21ms/step - loss: 1.5781 - accuracy: 0.2787 - val_loss: 1.5509 - val_accuracy: 0.3458\n","Epoch 2/5\n","230/230 [==============================] - 5s 21ms/step - loss: 1.5478 - accuracy: 0.3287 - val_loss: 1.5184 - val_accuracy: 0.4088\n","Epoch 3/5\n","230/230 [==============================] - 5s 20ms/step - loss: 1.5187 - accuracy: 0.3804 - val_loss: 1.4865 - val_accuracy: 0.5104\n","Epoch 4/5\n","230/230 [==============================] - 6s 26ms/step - loss: 1.4903 - accuracy: 0.4335 - val_loss: 1.4552 - val_accuracy: 0.5900\n","Epoch 5/5\n","230/230 [==============================] - 5s 21ms/step - loss: 1.4608 - accuracy: 0.4903 - val_loss: 1.4241 - val_accuracy: 0.6521\n","Training time: 0:00:25.638216\n","Test score: 1.4241150617599487\n","Test accuracy: 0.6521291732788086\n"]}],"source":["train_model(model2,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
